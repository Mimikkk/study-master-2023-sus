{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drzewa Wzmacniane Gradientowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import Node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "W tym zadaniu będziesz implementował algorytm Gradient Boosting Trees dla problemu regresji. Aby zaimplementować ten algorytm dostępny jest obiekt `Node` implementujący drzewo regresyjne. Jest to odpowiednio dostosowany obiekt, który implementowałeś w pierwszym zadaniu domowym. Możesz wykorzystać swoją własną implementację (i dostosować ją wg. opisu poniżej) lub skorzystać z implementacji w pliku `helpers`.\n",
    "\n",
    "W stosunku do poprzedniej implementacji obiekt ma pewne dodatkowe cechy, które umożliwią sprawniejszą implementację:\n",
    "- W konstruktorze `Node` jest teraz jeden obowiązkowy argument `calculate_leaf_value`, do którego należy wstawić funkcję, która jest wywoływana przez algorytm w momencie tworzenia liścia celem obliczenia jego wartości. W standardowym drzewie regresji algorytm tworzący liść oblicza jego wartość jako średnią wartość jego elementów. Jeśli chcielibyśmy uzyskać takie działanie, powinniśmy zaimplementować następującą funkcję:\n",
    "\n",
    "```python\n",
    "def mean_val_leaf(X, y, last_predicted):\n",
    "    return np.mean(y)\n",
    "\n",
    "tree = Node(calculate_leaf_value=mean_val_leaf)\n",
    "```\n",
    "Zwróć uwagę na parametry funkcji tworzącej liść: `X`, `y` charakteryzujące obiekty w liściu oraz `last_predicted` przechowujące aktualną predykcję klasyfikatora dla tych obiektów. Poprzez aktualną predykcję rozumiemy tu predykcję uzyskaną wszystkimi dotychczas stworzonymi klasyfikatorami bazowymi w GBT (czyli wynik osiągnięty pozostałymi drzewami niż to tworzone). Argument `last_predicted` na chwilę obecną wydaje się niepotrzebny, lecz będzie on potrzebny do realizacji zadania.\n",
    "\n",
    "- Dodatkowe argumenty obsługuje też funkcja ucząca model `fit(X, y, last_predicted, max_depth = None)` - która dostaje na wejście wcześniej wspomniane `last_predicted` oraz argument `max_depth` wstrzymujący budowę zbyt głębokich drzew. Innych mechanizmów pruningu niezaimplementowano, jeśli jednak takowe istnieją w Twojej implementacji, możesz je wykorzystać.\n",
    "\n",
    "Stwórz zbiór danych do regresji poniższym kodem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-5,5,100)\n",
    "y = 5 + X + np.sin(X) + np.random.normal(scale=0.1, size=100)\n",
    "plt.plot(X,y,'o')\n",
    "X = np.expand_dims(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ab39132c7a3a8f6a06d3d32f14d9b2",
     "grade": false,
     "grade_id": "cell-b262950717511628",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Zaimplementuj algorytm GBT dla błędu kwadratowego. Aby to zrobić należy uzupełnić w ogólnym pseudokodzie przedstawionym na zajęciach trzy elementy:\n",
    "- model początkowy $F_0(x)$ zwracający stałą wartość $v$ która optymalizuje błąd:\n",
    "$$F_0(x) = \\arg \\min_v \\sum_{i=1}^N L(y_i, v) $$\n",
    "- wzór na wartość ujemnego gradientu tj. pseudo-rezyduum:\n",
    "$$r_i  =  - \\frac{\\partial}{\\partial \\hat{y_i}} L(y_i, \\hat{y_i}) $$\n",
    "gdzie $\\hat{y_i}$ to aktualna predykcja klasyfikatora tj. w $m$-tej iteracji $\\hat{y_i}=F_m(x)$\n",
    "- wzór na wartość liścia $v$ optymalizujący funkcję celu całego modelu GBT\n",
    "$$v = \\arg \\min_v \\sum_{i=1}^{N_l} L(y_i, F_{m-1}(x_i) + v) $$\n",
    "Zwróć uwagę, że suma iteruje tylko po instancjach w liściu (${N_l}$ to liczba elementów w liściu).\n",
    "\n",
    "Wyznacz powyższe wartości (rozwiązania dla referencji poniżej komórki z kodem) i zaimplementuj algorytm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dda6f1fa44c0aafbc3a3038a204a44ac",
     "grade": false,
     "grade_id": "cell-ac1ee19f7896664e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GBTRegressor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trees = [] # Lista kolejnych drzew (Node) klasyfikatora\n",
    "        self.initial_model = None # Model początkowy, zwracający stałą wartość \n",
    "                                  #(zmienna po prostu przechowuje tę wartość)\n",
    "        \n",
    "    def calc_leaf(self, X, y, last_predicted):\n",
    "        \"\"\"\n",
    "        Funkcja do wykorzystania jako argument klasyfikatora bazowego Node\n",
    "        \"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "        \n",
    "    def fit(self, X, y, M = 100, max_depth = 1):\n",
    "        \"\"\" \n",
    "        Funckja trenująca model GBT o M klasyfikatora bazowych\n",
    "        z maksymalną głębkością pojedynczego drzewa max_depth\n",
    "        \"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Zwrócenie predykcji dla obiektów testowych X\n",
    "        \"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetestuj działanie algorytmu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-5, 5, num=500)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "gbt = GBTRegressor()\n",
    "gbt.fit(X,y)\n",
    "y_pred = gbt.predict(X_test)\n",
    "plt.plot(X,y,'o')\n",
    "plt.plot(X_test,y_pred,'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj wynik modelu z odpowednio 1, 2, 5, 10 i 100 klasyfikatorami bazowami. Za klasfikator bazowy przyjmij decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ade20515d1f06a84104be6a16716aaa0",
     "grade": false,
     "grade_id": "cell-8cf805d58cd501f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak zmieniają się wartości redyduów w kilku początkowych iteracjach GBT. Narysuj wykresy $x$ vs $y-\\hat{y}$ - zwróć uwagę, że tak właśnie wyglądają zbiory na których uczą się kolejne klasyfikatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dc4e4f10861d42ad678489a7bfd866a",
     "grade": false,
     "grade_id": "cell-c812e92b907ca3a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Odpowiedzi:*\n",
    "- model początkowy $F_0(x)$\n",
    "$$F_0(x) = \\arg \\min_v \\sum_{i=1}^N L(y_i, v) = \\frac{1}{2} \\sum_{i=1}^N (y_i- v)^2 $$\n",
    "wartość ta to oczywiście średnia arytmetyczna $v = \\frac{1}{n} \\sum_{i=1}^N y_i$. (Upewnij się, że to rozumiesz poprzez policzenie pochodnej i przyrównanie jej do 0).\n",
    "- wzór na wartość ujemnego gradientu tj. pseudo-rezyduum \n",
    "$$r_i  =  - \\frac{\\partial}{\\partial \\hat{y_i}} L(y_i, \\hat{y_i}) = - \\frac{\\partial}{\\partial \\hat{y_i}} \\frac{1}{2}(y_i- \\hat{y_i})^2$$\n",
    "Co po przekształceniach wykorzystujących regułę łańcuchową (\"pochodna zewnętrzna razy pochodna wewnętrzna\"):\n",
    "$$r_i  = -\\frac{1}{2} 2(y_i- \\hat{y_i})\\frac{\\partial}{\\partial \\hat{y_i}} (y_i- \\hat{y_i}) \n",
    "= -(y_i- \\hat{y_i})\\cdot(-1)\n",
    "= y_i- \\hat{y_i}  $$\n",
    "- wzór na wartość liścia $v$ optymalizujący funkcję celu\n",
    "$$v = \\arg \\min_v \\sum_{i=1}^{N_l} L(y_i, F_{m-1}(x_i) + v) = \\frac{1}{2} \\sum_{i=1}^{N_l} (y_i- F_{m-1}(x_i) - v)^2 $$\n",
    "Co można dalej obliczyć poprzez przyrównanie pochodnej do 0 lub poprzez zauważenie że jest to w naszej sytuacji ten sam wzór co dla modelu początkowego gdzie $y_i$ zostało zastępione $y_i- F_{m-1}(x_i)=r_i$. W związku z tym wartość liścia to $v = \\frac{1}{n} \\sum_{i=1}^N r_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Zadanie 2\n",
    "Zaimplementuj GBT dla problemu klasyfikacji binarnej, który będzie optymalizował błąd regresji logicznej tj. entropię krzyżową wyrażoną wzorem:\n",
    "$$L(y_i, \\hat{p_i}) = y_i \\log \\hat{p_i} +  (1-y_i) \\log (1-\\hat{p_i}) $$\n",
    "gdzie $y_i\\in \\{0,1\\}$ to prawdziwa wartość klasy a $\\hat{p_i}$ to predykcja klasyfikatora dla $i$-tego elementu.\n",
    "\n",
    "- Zauważ, że GBT wykorzystuje drzewa regresji, które - choć modyfikujemy im sposób obliczania liści - nadal tworzą podziały dla miary SSE. Aby wykorzystać GBT do problemu klasyfikacji, należy zastanowić się jak możemy wykorzystać regresor do klasyfikacji. Ten problem rozwiązywaliśmy już wcześniej przy omawianiu regresji logistycznej, gdzie tworzyliśmy klasyfikator z modelu regresji liniowej. Przypomnijmy, że w regresji logistycznej model regresji liniowej służy do predykcji logitu prawdopodobieństwa klasy (który przypomnijmy ma zakres wartości od $-\\infty$ do $\\infty$)\n",
    "$$\\text{logit}(p_x) = \\ln \\frac{p_x}{1-p_x}=w^Tx+b$$\n",
    "Podobnie w GBT należy skonstruować model regresji do przewidywania wartości $\\text{logit}(p_x)$, a jedynie przy predykcji (lub kiedy jest to wygodne) transformować go do prawdopodobieństwa klasy funkcją sigmoidalną $p_x  = \\frac{1}{1+e^{- \\text{logit}(p_x)}}  $\n",
    "\n",
    "**Zadania**\n",
    "\n",
    "1. Powyższy zapis funkcji celu $L(y_i, \\hat{p_i})$ jest wyrażony w zależności od prawdopodobieństwa klasy, a nie wartości logitu $L(y_i, \\text{logit}(\\hat{p_i}))$. Przekształć wzór na funkcję celu, aby jej argumentem był logit. Zwróć uwagę, że model regresji będzie przewidywał właśnie logit, więc przy wyznaczaniu elementów algorytmu GBT należy liczyć np. pochodne tej właśnie przekształconej funkcji.\n",
    "\n",
    "    Zapisz wzór na tę funkcję w komórce poniżej (np. w komentarzu, nie musisz implementować).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75c63b56480dfd92cd8d4eeb54e90756",
     "grade": true,
     "grade_id": "cell-7cd53887e8606afb",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Zacznijmy uzupełniać w ogólnym pseudokodzie przedstawionym na zajęciach brakujące elementy. Wyznacz model początkowy $F_0(x)$ zwracający stałą wartość $v$ która optymalizuje błąd:\n",
    "$$F_0(x) = \\arg \\min_v \\sum_{i=1}^N L(y_i, v) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "445fef5807bdbfe7e4f369ca7276c48a",
     "grade": true,
     "grade_id": "cell-a31084a299567b73",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wyznacz wzór na wartość ujemnego gradientu tj. pseudo-rezyduum:\n",
    "$$r_i  =  - \\frac{\\partial}{\\partial \\hat{y_i}} L(y_i, \\hat{y_i}) $$\n",
    "Uwaga: na samym końcu, aby wzór uzykał prostszą formę, możesz zamienić w nim wartości logitów z powrotem na prawdopodobieństwa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f524aa1cd9ba572fa5bb0468913b2f05",
     "grade": true,
     "grade_id": "cell-dc1c684d06b2f089",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Wzór na wartość liścia $v$ optymalizujący funkcję celu całego modelu GBT\n",
    "$$v = \\arg \\min_v \\sum_{i=1}^{N_l} L(y_i, F_{m-1}(x_i) + v) $$\n",
    "niestety nie jest prosty do wyznaczenia w tym przypadku. Stosuje się przybliżenie Taylora drugiego rzedu tej funkcji i wtedy optimum ma postać:\n",
    "$$v = \\frac{-\\sum_{i=1}^{N_L} L_i' }{\\sum_{i=1}^{N_L} L_i''}$$\n",
    "gdzie $L_i'$ i $L_i''$ to skrócony zapis pierwszej i drugiej pochodnej policzonej po funkcji straty dla $i$-tego elementu. Ponieważ $r_i=-L_i'$ to licznik przyjmuje postać $\\sum_{i=1}^{N_L} r_i $. Wyznacz cały wzór.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d14f23700f926ba83983b920e24ddf2",
     "grade": true,
     "grade_id": "cell-193f2492d4d4966d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystując uzyskane wyniki zaimplementuj algorytm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10d00855082fd2660203fa55f0192e12",
     "grade": false,
     "grade_id": "cell-49860f3147428f2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "# Wskazówka: scipy.special.expit() implemenuje funkcję sigmoidalną\n",
    "\n",
    "class GBTClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.trees = []\n",
    "        self.initial_model = None\n",
    "        \n",
    "    def calc_leaf(self, X, y, last_predicted):\\\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    \n",
    "    def fit(self,X,y, M = 100, max_depth=1):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "            \n",
    "    def predict(self, X):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1d9196e9bcfdf96ee378612cc201a26",
     "grade": false,
     "grade_id": "cell-5e0ac3aa1b4504ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Przetestuj swoją implementację na zbinaryzowanym zbiorze `iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "y[y==2] = 0 # Sprowadzenie problemu do klasyfikacji binarnej\n",
    "\n",
    "# Kod rysowania\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "clf = GBTClassifier()\n",
    "clf.fit(X,y)\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj granice decyzji klasyfikatora dla 10, 20, 50 i 100 iteracji algorytmu dla klasyfikatora bazowego o maksymalnej głębokości 1 i 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c9df781f941bc16c82c578aab5cc772",
     "grade": false,
     "grade_id": "cell-23a29d3c5f38cd20",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "1. Przeanalizuj wyniki powyższego eksperymentu i określ w których sytuacjach nastąpiło przeuczenie.\n",
    "2. W jaki sposób zaimplementować GBT dla problemu klasyfikacji wieloklasowej?\n",
    "3. W powyższym problemie który z klasyfikatorów bazowych (o jakiej max. głębokości) poradził sobie lepiej? Czy jest sens stosować w tym problemie drzewa o głębokości większej niż testowana (tj. 2). Odpowiedź uzasadnij.\n",
    "4. Dodaj do implementacji parametr $\\eta$ i przetestuj kilka jego wartości. Pamętaj, że $\\eta$ powinno być wykorzystywane nie tylko w funkcji `fit`, ale także `predict` - dlaczego?\n",
    "\n",
    "Odpowiedź na trzecią kropkę umieść poniżej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df47fa73f642026fe4a85f7478c67e29",
     "grade": true,
     "grade_id": "cell-73f4aa08b5f50458",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3\n",
    "GBT jest bardzo popularnym algorytmem głównie dzięki bardzo efektywnym implementacjom potrafiącym sobie radzić z dużymi zbiorami danych. W tym ćwiczeniu Twoim zadaniem jest nauczenie się podstaw obsługi biblioteki `catboost`, którą powinieneś zainstalować.\n",
    "\n",
    "Wczytanie zbioru danych `iris` z poprzedniego zadania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "y[y==2] = 0 # Sprowadzenie problemu do klasyfikacji binarnej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(logging_level='Silent')\n",
    "model.fit(X, y, eval_set=(X, y), plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowy kod ewaluuje działanie algorytmu na części uczącej. Podziel zbiór na część uczącą i testową i ponownie uruchom algorytm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdybyś porównał wartość funkcji straty osiągniętej przez catboost z wartością uzyskaną Twoją implementacją na zbiorze uczącym przy analogicznej liczbie drzew (domyślnie w `GBTClassifier` $M=100$) to wartość ta będzie najprawdopodobniej niższa dla... Twojej implementacji. Dlaczego? Czy to oznacza, że - pomijając aspekt wydajności obliczeń - Twoja metoda działa lepiej niż catboost?\n",
    "\n",
    "*Dla chętnych*: porównaj wartość funkcji straty osiągniętej przez catboost z Twoją implementacją z zadania 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbf29a0ecded7277ba034534aff80a02",
     "grade": true,
     "grade_id": "cell-c8a35a322d9b4893",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimportuj dowolny większy i bardziej wymagający zbiór danych. Ćwiczenie możesz wykonać na [dowolnym zbiorze danych](https://catboost.ai/docs/concepts/python-reference_datasets.html) - ładowanie zbioru może trochę potrwać. Jeśli masz problemy sprzętowe z operowaniem na dużym zbiorze danych to jest też dostępny zbiór `titanic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import datasets\n",
    "\n",
    "train_df, test_df = datasets.adult()\n",
    "print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4008306dfc33868b6dac96ffdf560f82",
     "grade": false,
     "grade_id": "cell-05da6f295da51eef",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Spróbuj osiągnąć jak najlepszy wynik na wybranym zbiorze poprzez tuning parametrów. Ważne parametry uczenia zostały opisane [tutaj](https://catboost.ai/docs/concepts/python-reference_parameters-list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jakie ustawienia parametrów dały najlepszy wynik na wybranym zbiorze danych? Które z parametrów algorytmu miały najsilniejszy wpływ na ostateczny wynik?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uwaga** Przed wysłaniem zadania domowego wróć do komórki w której ładowałeś zbiór danych i zakomentuj ją, aby niewykonała się ona na sprawdzarce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
